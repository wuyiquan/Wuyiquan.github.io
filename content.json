{"meta":{"title":"Wu's Blog","subtitle":"不定时更新","description":"一个帅哥","author":"Yiquan Wu","url":"http://yoursite.com"},"pages":[],"posts":[{"title":"","slug":"TF-IDF","date":"2019-01-11T07:53:50.057Z","updated":"2019-01-19T07:26:49.300Z","comments":true,"path":"2019/01/11/TF-IDF/","link":"","permalink":"http://yoursite.com/2019/01/11/TF-IDF/","excerpt":"","text":"最近做的项目需要无监督的抽取一些标签，本来想粗暴的用词频来抽取，后来同事说可以试试TF-IDF，我就自学了一下，现做摘记如下： TF-IDF全称为Term Frequency - Inverse Documentation Frequency，即词频-逆文档频率，是当前非常常用的一种文本特征的提取方法，在文本信息检索，语意抽取等自然语言处理（NLP）中广泛应用。TF:即词频，定义有很多，这里定义如下：$$TF(w)=\\frac{c_{w}}{c_{total}}{\\tag 1}$$其中，$c_{w}$指在目标文档中某个词出现的次数，$c_{total}$指目标文档所有词出现的次数之和（也就是一共有几个词）。 我一开始的方法，就是指用TF做排序，然后人工去过滤一些stop word，但这样效率太低了。 IDF:逆文档频率，这个逆文档好像ads课上讲过，意思是有一个词，然后去找哪些文档里出现了这个词，这与常识中的查找思路正好相反，因此有个逆，其定义如下：$$IDF(w)=log\\frac{N_{total}+1}{N_{w}+1}{\\tag 2}$$其中，$N_{total}$指语料库里一共有多少文档，$N_{w}​$指某个词一共出现在了多少文档。这里之所以加1，是为了防止分母为0的情况出现。 TF-IDF就是TF*IDF，然后排序，非常的简单。 它的优点是可以过滤掉一些在所有文档中都频繁出现的词语，即stop word，同时保留我们需要的一些能够代表文档的关键词。 缺点是可能某一类文档占总语料库的比例较大时，可能会使一些关键词被误认为是stop word. python中，有一个Scikit-learn包，包含了tf-idf方法，我们可以直接拿来用 主要用到的函数有两个： CountVectorizer，用来将语料库中的词语转化为词频矩阵。 TfidfTransformer，根据词频得到TF-IDF。 是","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2019-01-11T07:22:47.248Z","updated":"2019-01-11T07:22:47.248Z","comments":true,"path":"2019/01/11/hello-world/","link":"","permalink":"http://yoursite.com/2019/01/11/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}